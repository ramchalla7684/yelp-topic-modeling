{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ram/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ram/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "import numpy\n",
    "import matplotlib.pyplot as plot\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet,  words, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import multiprocessing\n",
    "from atpbar import atpbar\n",
    "from atpbar import register_reporter, find_reporter, flush\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pympler import asizeof\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags: r - adverb, j - adjective, n - noun, v - verb, \n",
    "def nouns(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean(tokens)\n",
    "    \n",
    "    _nouns = [token for token, pos in pos_tag(tokens) if pos[0] == 'N']\n",
    "    return _nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags: r - adverb, j - adjective, n - noun, v - verb, \n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    re_word = re.compile('^[a-z]+$')\n",
    "    \n",
    "    tokens = []\n",
    "    for token, pos in pos_tag(word_tokenize(text)):\n",
    "        \n",
    "        if not bool(re_word.match(token)):\n",
    "            continue\n",
    "\n",
    "        lemma = None\n",
    "        if pos[0] in ['A', 'N', 'R', 'V']:\n",
    "            lemma = lemmatize(token, pos[0].lower())\n",
    "        else:\n",
    "            if token not in english_words and wordnet.morphy(token) is None:\n",
    "                continue\n",
    "            lemma = token\n",
    "            \n",
    "        tokens.append(lemma)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokens):\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mediocre', 'food', 'loud', 'filet', 'mignon', 'poivre', 'order', 'medium', 'virtually', 'raw', 'one', 'end', 'steak', 'hint', 'sauce', 'bland', 'famous', 'fry', 'greasy', 'strip', 'potato', 'skin', 'house', 'salad', 'decent', 'service', 'kitchen', 'painfully', 'slow', 'minute', 'receive', 'entree', 'impressed', 'per', 'person', 'back']\n",
      "\n",
      "['mediocre', 'food', 'filet', 'mignon', 'poivre', 'order', 'medium', 'end', 'steak', 'hint', 'sauce', 'fry', 'strip', 'potato', 'skin', 'house', 'service', 'minute', 'entree', 'person']\n"
     ]
    }
   ],
   "source": [
    "text = 'Mediocre food, very loud.  Filet mignon au poivre ordered \"medium\" was virtually raw on one end of the steak with only a hint of sauce.  Very bland. The \"famous\" fries are greasy strips of potato skins.  House salad was decent.  Service was OK, but the kitchen was painfully slow. 45 minutes to receive entree.  Not impressed.  $100 per person.  Won\\'t be back.'\n",
    "\n",
    "tokens = tokenize(text)\n",
    "tokens = clean(tokens)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "print(nouns(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/44073393/parallelizing-loading-data-from-mongodb-into-python\n",
    "\n",
    "def multiprocess_cursor(n_cores, collection_size, process_cursor):\n",
    "    batch_size = round(collection_size/n_cores+0.5)\n",
    "    skips = range(0, n_cores*batch_size, batch_size)\n",
    "\n",
    "    reporter = find_reporter()\n",
    "    processes = [multiprocessing.Process(target=process_cursor, args=(skip_n,batch_size, reporter)) for skip_n in skips]\n",
    "\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tokens (nouns, verbs, adverbs, adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cursor(skip_n, limit_n, reporter):\n",
    "    print('Starting process',skip_n//limit_n,'...')\n",
    "    \n",
    "    register_reporter(reporter)\n",
    "    \n",
    "    db =  MongoClient(port=27017).yelp\n",
    "    cursor = db.reviews.find({}, {'text': 1}).sort('_id', 1).skip(skip_n).limit(limit_n)\n",
    "\n",
    "    reviews = []\n",
    "    for review in cursor:\n",
    "        reviews.append(review)\n",
    "        \n",
    "    for i in atpbar(range(len(reviews)), name=str(skip_n//limit_n)):\n",
    "        review = reviews[i]\n",
    "        if 'text' in review:\n",
    "            tokens = tokenize(review['text'])\n",
    "            tokens = clean(tokens)\n",
    "            db.reviews.update_one({'_id': review['_id']}, {'$set': {'tokens': tokens}})\n",
    "\n",
    "    print('Completed process',skip_n//limit_n,'...')\n",
    "\n",
    "n_cores = 8\n",
    "collection_size = 6685900\n",
    "batch_size = round(collection_size/n_cores+0.5)\n",
    "skips = range(0, n_cores*batch_size, batch_size)\n",
    "\n",
    "# progress = tqdm(total=6685900, leave=True, position=0)\n",
    "\n",
    "reporter = find_reporter()\n",
    "processes = [multiprocessing.Process(target=process_cursor, args=(skip_n,batch_size, reporter)) for skip_n in skips]\n",
    "\n",
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process 0 ...\n",
      "Starting process 1 ...\n",
      "Starting process 2 ...\n",
      "Starting process 3 ...\n",
      "Starting process 4 ...\n",
      "Starting process 5 ...\n",
      "Starting process 6 ...\n",
      "Starting process 7 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28913960662b4bb9aad4947dc091b3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed process 7 ...\n",
      "Completed process 4 ...\n",
      "Completed process 6 ...\n",
      "Completed process 0 ...\n",
      "Completed process 2 ...\n",
      "Completed process 1 ...\n",
      "Completed process 3 ...\n",
      "Completed process 5 ...\n"
     ]
    }
   ],
   "source": [
    "def process_cursor(skip_n, limit_n, reporter):\n",
    "    print('Starting process',skip_n//limit_n,'...')\n",
    "    \n",
    "    register_reporter(reporter)\n",
    "    \n",
    "    db =  MongoClient(port=27017).yelp\n",
    "    cursor = db.reviews_sub_2.find({}, {'text': 1}).sort('_id', 1).skip(skip_n).limit(limit_n)\n",
    "\n",
    "    reviews = []\n",
    "    for review in cursor:\n",
    "        reviews.append(review)\n",
    "        \n",
    "    for i in atpbar(range(len(reviews)), name=str(skip_n//limit_n)):\n",
    "        review = reviews[i]\n",
    "        if 'text' in review:\n",
    "            _nouns = nouns(review['text'])\n",
    "            db.reviews_sub_2.update_one({'_id': review['_id']}, {'$set': {'nouns': _nouns}})\n",
    "\n",
    "    print('Completed process',skip_n//limit_n,'...')\n",
    "\n",
    "\n",
    "multiprocess_cursor(n_cores=8, collection_size=282415, process_cursor=process_cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoClient = MongoClient(port=27017)\n",
    "db = mongoClient.yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load extracted nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282415/282415 [00:03<00:00, 4590.75it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "progress = tqdm(total=282415, leave=True, position=0)\n",
    "cursor = db.reviews_sub_2.find({}, {'nouns': 1}).sort('_id', 1)\n",
    "for doc in cursor:\n",
    "    progress.update(1)\n",
    "    if 'nouns' in doc:\n",
    "        tokenized_docs.append(doc['nouns'])\n",
    "\n",
    "progress.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 282415/282415 [00:16<00:00, 4590.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 523 ms, total: 18.9 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
    "\n",
    "dictionary.save('dictionary_sub_nouns_2.pkl')\n",
    "with open('corpus_sub_nouns_2.pkl', 'wb') as file:\n",
    "    pickle.dump(corpus, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = None\n",
    "corpus = None\n",
    "\n",
    "with open('dictionary_sub_nouns_2.pkl', 'rb') as file:\n",
    "    dictionary = pickle.load(file)\n",
    "    \n",
    "with open('corpus_sub_nouns_2.pkl', 'rb') as file:\n",
    "    corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6e5c85cdf707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "logging.shutdown()\n",
    "reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 1min 2s, total: 4min 35s\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_topics = 20\n",
    "\n",
    "model_20 = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=40)\n",
    "model_20.save('model_sub_nouns_2_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20 = LdaModel.load('model_sub_nouns_2_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6119475232059287\n",
      "CPU times: user 11.1 s, sys: 1.4 s, total: 12.5 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coherence_model = CoherenceModel(model=model_20, texts= tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
    "print(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = model.num_topics\n",
    "n_terms = 30\n",
    "display(model.show_topics(n_topics, n_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = model.num_topics\n",
    "n_terms = 30\n",
    "for i in range(n_topics):\n",
    "    display(model.show_topic(i, n_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hard to beat this location for table side entertainment\"\n",
    "tokens = tokenize(text)\n",
    "tokens = clean(tokens)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "bow = dictionary.doc2bow(tokens)\n",
    "print(bow)\n",
    "print(model.get_document_topics(bow, minimum_probability=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(text):\n",
    "#     print(text)\n",
    "    topic_terms = []\n",
    "    \n",
    "    tokens = nouns(text)\n",
    "    _tokens_set = set(tokens)\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topics = model.get_document_topics(bow, minimum_probability=0.1)\n",
    "    topics = sorted(topics, key=lambda a: -a[1])\n",
    "    for topic_idx, odds in topics:\n",
    "#         n_terms = odds > 0.5 and 6 or 3\n",
    "#         terms = set([term for term, _ in model.show_topic(topic_idx, 50)])\n",
    "        topic_terms.extend([term for term, _ in model.show_topic(topic_idx, 50) if term in _tokens_set])\n",
    "#         topic_terms.extend([token for token in tokens if token in terms])\n",
    "#         topic_terms.extend([term for term, _ in model.show_topic(topic_idx, n_terms)])\n",
    "        \n",
    "    return set(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, stars):\n",
    "    compound = sentimentIntensityAnalyzer.polarity_scores(text)['compound'],\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    score = compound[0] == 0 and polarity or compound[0]\n",
    "    score = (score + numpy.interp(stars, [1, 5], [-1, 1]))/2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(text, stars):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    sentiment_scores = {}\n",
    "    for sentence in sentences:\n",
    "        terms = get_topic_terms(sentence)\n",
    "#         print(terms)\n",
    "        sentiment = get_sentiment(sentence, stars)\n",
    "#         print(sentiment)\n",
    "        for term in terms:\n",
    "            if term not in sentiment_scores:\n",
    "                sentiment_scores[term] = []\n",
    "            sentiment_scores[term].append(sentiment)\n",
    "    return sentiment_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentiment_scores(sentiment_scores):\n",
    "    return {k: numpy.mean(v) for k,v in sentiment_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 s, sys: 215 ms, total: 5.36 s\n",
      "Wall time: 5.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cursor = db.reviews_sub_2.find({}, {'text': 1, 'stars': 1}).sort('_id', 1).limit(100)\n",
    "for doc in cursor:\n",
    "    if 'text' in doc and 'stars' in doc:\n",
    "        review = doc['text']\n",
    "        stars = doc['stars']\n",
    "        sentiment_scores = get_sentiment_scores(review, stars)\n",
    "#         print(sentiment_scores.keys())\n",
    "#         print()\n",
    "        sentiment_scores = avg_sentiment_scores(sentiment_scores)\n",
    "#         print(sentiment_scores)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and store topics, keywords, and sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.reviews_sub_2_2.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process 0 ...\n",
      "Starting process 1 ...\n",
      "Starting process 2 ...\n",
      "Starting process 3 ...\n",
      "Starting process 4 ...\n",
      "Starting process 5 ...\n",
      "Starting process 6 ...\n",
      "Starting process 7 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e1099d39684849b34c1f794d6d91df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving....\n",
      "Completed process 7 ...\n",
      "Saving....\n",
      "Completed process 6 ...\n",
      "Saving....\n",
      "Completed process 4 ...\n",
      "Saving....\n",
      "Saving....\n",
      "Completed process 1 ...\n",
      "Completed process 0 ...\n",
      "Saving....\n",
      "Completed process 2 ...\n",
      "Saving....\n",
      "Completed process 3 ...\n",
      "Saving....\n",
      "Completed process 5 ...\n"
     ]
    }
   ],
   "source": [
    "def process_cursor(skip_n, limit_n, reporter):\n",
    "    print('Starting process',skip_n//limit_n,'...')\n",
    "    \n",
    "    register_reporter(reporter)\n",
    "    \n",
    "    db =  MongoClient(port=27017).yelp\n",
    "    cursor = db.reviews_sub_2.find({}).sort('_id', 1).skip(skip_n).limit(limit_n)\n",
    "\n",
    "    reviews = []\n",
    "    for review in cursor:\n",
    "        reviews.append(review)\n",
    "        \n",
    "    for i in atpbar(range(len(reviews)), name=str(skip_n//limit_n)):\n",
    "        review = reviews[i]\n",
    "        if 'text' in review and 'stars' in review:\n",
    "            sentiment_scores = get_sentiment_scores(review['text'], review['stars'])\n",
    "            sentiment_scores = avg_sentiment_scores(sentiment_scores)\n",
    "            review['keywords'] = sentiment_scores\n",
    "    print(\"Saving....\")\n",
    "    db.reviews_sub_2_2.insert_many(reviews)\n",
    "\n",
    "    print('Completed process',skip_n//limit_n,'...')\n",
    "\n",
    "\n",
    "db.reviews_sub_2_2.drop()\n",
    "multiprocess_cursor(n_cores=8, collection_size=282415, process_cursor=process_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
