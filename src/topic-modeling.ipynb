{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import numpy\n",
    "import matplotlib.pyplot as plot\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet,  words, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pympler import asizeof\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags: r - adverb, j - adjective, n - noun, v - verb, \n",
    "def nouns(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean(tokens)\n",
    "    \n",
    "    _nouns = [token for token, pos in pos_tag(tokens) if pos[0] == 'N']\n",
    "    return _nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags: r - adverb, j - adjective, n - noun, v - verb, \n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    re_word = re.compile('^[a-z]+$')\n",
    "    \n",
    "    tokens = []\n",
    "    for token, pos in pos_tag(word_tokenize(text)):\n",
    "        \n",
    "        if not bool(re_word.match(token)):\n",
    "            continue\n",
    "\n",
    "        lemma = None\n",
    "        if pos[0] in ['A', 'N', 'R', 'V']:\n",
    "            lemma = lemmatize(token, pos[0].lower())\n",
    "        else:\n",
    "            if token not in english_words and wordnet.morphy(token) is None:\n",
    "                continue\n",
    "            lemma = token\n",
    "            \n",
    "        tokens.append(lemma)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokens):\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Mediocre food, very loud.  Filet mignon au poivre ordered \"medium\" was virtually raw on one end of the steak with only a hint of sauce.  Very bland. The \"famous\" fries are greasy strips of potato skins.  House salad was decent.  Service was OK, but the kitchen was painfully slow. 45 minutes to receive entree.  Not impressed.  $100 per person.  Won\\'t be back.'\n",
    "\n",
    "tokens = tokenize(text)\n",
    "tokens = clean(tokens)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "print(nouns(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tokens (nouns, verbs, adverbs, adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/44073393/parallelizing-loading-data-from-mongodb-into-python\n",
    "import multiprocessing\n",
    "from atpbar import atpbar\n",
    "from atpbar import register_reporter, find_reporter, flush\n",
    "\n",
    "def process_cursor(skip_n, limit_n, reporter):\n",
    "    print('Starting process',skip_n//limit_n,'...')\n",
    "    \n",
    "    register_reporter(reporter)\n",
    "    \n",
    "    db =  MongoClient(port=27017).yelp\n",
    "    cursor = db.reviews.find({}, {'text': 1}).sort('_id', 1).skip(skip_n).limit(limit_n)\n",
    "\n",
    "    reviews = []\n",
    "    for review in cursor:\n",
    "        reviews.append(review)\n",
    "        \n",
    "    for i in atpbar(range(len(reviews)), name=str(skip_n//limit_n)):\n",
    "        review = reviews[i]\n",
    "        if 'text' in review:\n",
    "            tokens = tokenize(review['text'])\n",
    "            tokens = clean(tokens)\n",
    "            db.reviews.update_one({'_id': review['_id']}, {'$set': {'tokens': tokens}})\n",
    "\n",
    "    print('Completed process',skip_n//limit_n,'...')\n",
    "\n",
    "n_cores = 8\n",
    "collection_size = 6685900\n",
    "batch_size = round(collection_size/n_cores+0.5)\n",
    "skips = range(0, n_cores*batch_size, batch_size)\n",
    "\n",
    "# progress = tqdm(total=6685900, leave=True, position=0)\n",
    "\n",
    "reporter = find_reporter()\n",
    "processes = [multiprocessing.Process(target=process_cursor, args=(skip_n,batch_size, reporter)) for skip_n in skips]\n",
    "\n",
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/44073393/parallelizing-loading-data-from-mongodb-into-python\n",
    "import multiprocessing\n",
    "from atpbar import atpbar\n",
    "from atpbar import register_reporter, find_reporter, flush\n",
    "\n",
    "def process_cursor(skip_n, limit_n, reporter):\n",
    "    print('Starting process',skip_n//limit_n,'...')\n",
    "    \n",
    "    register_reporter(reporter)\n",
    "    \n",
    "    db =  MongoClient(port=27017).yelp\n",
    "    cursor = db.reviews_sub.find({}, {'text': 1}).sort('_id', 1).skip(skip_n).limit(limit_n)\n",
    "\n",
    "    reviews = []\n",
    "    for review in cursor:\n",
    "        reviews.append(review)\n",
    "        \n",
    "    for i in atpbar(range(len(reviews)), name=str(skip_n//limit_n)):\n",
    "        review = reviews[i]\n",
    "        if 'text' in review:\n",
    "            _nouns = nouns(review['text'])\n",
    "            db.reviews_sub.update_one({'_id': review['_id']}, {'$set': {'nouns': _nouns}})\n",
    "\n",
    "    print('Completed process',skip_n//limit_n,'...')\n",
    "\n",
    "n_cores = 8\n",
    "collection_size = 767985\n",
    "batch_size = round(collection_size/n_cores+0.5)\n",
    "skips = range(0, n_cores*batch_size, batch_size)\n",
    "\n",
    "reporter = find_reporter()\n",
    "processes = [multiprocessing.Process(target=process_cursor, args=(skip_n,batch_size, reporter)) for skip_n in skips]\n",
    "\n",
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoClient = MongoClient(port=27017)\n",
    "db = mongoClient.yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load extracted nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "progress = tqdm(total=767985, leave=True, position=0)\n",
    "cursor = db.reviews_sub.find({}, {'nouns': 1}).sort('_id', 1)\n",
    "for doc in cursor:\n",
    "    progress.update(1)\n",
    "    if 'nouns' in doc:\n",
    "        tokenized_docs.append(doc['nouns'])\n",
    "\n",
    "progress.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
    "\n",
    "dictionary.save('dictionary_sub_nouns.pkl')\n",
    "with open('corpus_sub_nouns.pkl', 'wb') as file:\n",
    "    pickle.dump(corpus, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = None\n",
    "corpus = None\n",
    "\n",
    "with open('dictionary_sub_nouns.pkl', 'rb') as file:\n",
    "    dictionary = pickle.load(file)\n",
    "    \n",
    "with open('corpus_sub_nouns.pkl', 'rb') as file:\n",
    "    corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 20\n",
    "\n",
    "model_20 = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=25)\n",
    "model_20.save('model_sub_nouns_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20 = LdaModel.load('model_sub_nouns_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "coherence_model = CoherenceModel(model=model_20, texts= tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
    "print(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = model.num_topics\n",
    "n_terms = 30\n",
    "display(model.show_topics(n_topics, n_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = model.num_topics\n",
    "n_terms = 30\n",
    "for i in range(n_topics):\n",
    "    display(model.show_topic(i, n_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hard to beat this location for table side entertainment\"\n",
    "tokens = tokenize(text)\n",
    "tokens = clean(tokens)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "bow = dictionary.doc2bow(tokens)\n",
    "print(bow)\n",
    "print(model.get_document_topics(bow, minimum_probability=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(text):\n",
    "    print(text)\n",
    "    topic_terms = []\n",
    "    \n",
    "    tokens = nouns(text)\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topics = model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    topics = sorted(topics, key=lambda a: -a[1])\n",
    "    for topic_idx, odds in topics:\n",
    "        n_terms = odds > 0.5 and 6 or 3\n",
    "        topic_terms.extend([term for term, _ in model.show_topic(topic_idx, 100) if term in tokens])\n",
    "#         topic_terms.extend([term for term, _ in model.show_topic(topic_idx, n_terms)])\n",
    "        \n",
    "    return set(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, stars):\n",
    "    compound = sentimentIntensityAnalyzer.polarity_scores(text)['compound'],\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    score = compound[0] == 0 and polarity or compound[0]\n",
    "    score = (score + numpy.interp(stars, [1, 5], [-1, 1]))/2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(text, stars):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    sentiment_scores = {}\n",
    "    for sentence in sentences:\n",
    "        terms = get_topic_terms(sentence)\n",
    "#         print(terms)\n",
    "        sentiment = get_sentiment(sentence, stars)\n",
    "#         print(sentiment)\n",
    "        for term in terms:\n",
    "            if term not in sentiment_scores:\n",
    "                sentiment_scores[term] = []\n",
    "            sentiment_scores[term].append(sentiment)\n",
    "    return sentiment_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = tqdm(total=767985, leave=True, position=0)\n",
    "cursor = db.reviews_sub.find({}, {'text': 1, 'stars': 1}).sort('_id', 1).limit(10)\n",
    "for doc in cursor:\n",
    "    progress.update(1)\n",
    "    if 'text' in doc and 'stars' in doc:\n",
    "        review = doc['text']\n",
    "        stars = doc['stars']\n",
    "        sentiment_scores = get_sentiment_scores(review, stars)\n",
    "        print(sentiment_score)\n",
    "        print()\n",
    "\n",
    "progress.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
